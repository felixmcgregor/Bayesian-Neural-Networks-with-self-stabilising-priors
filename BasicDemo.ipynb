{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUt8t6gv8HK"
   },
   "source": [
    "# Self stabilising priors for robust Bayesian deep learning\n",
    "\n",
    "This notebook is designed to demonstrate the principle to give the idea and a basic implementation to make it accessible. We demonstrate accelerated training with an experiment on MNIST.\n",
    "\n",
    "\n",
    "This notebook is based on  https://github.com/senya-ashukha/sparse-vd-pytorch/blob/master/svdo-solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"intuition1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVnbfF7pwbeH"
   },
   "source": [
    "### Installation and to run on google colab  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "v2hTKqOQwTkB",
    "outputId": "56335e7f-c6d2-40a9-9eb2-971bfcf745b9"
   },
   "outputs": [],
   "source": [
    "# Logger\n",
    "#!pip install tabulate -q\n",
    "#from google.colab import files\n",
    "#src = list(files.upload().values())[0]\n",
    "#open('logger.py','wb').write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiCiVLaJv8HV"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICMEDWnov8HW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "width = 256\n",
    "input_shape = 28*28\n",
    "output_size = 10\n",
    "batch_size = 100\n",
    "init_var = 0.001\n",
    "\n",
    "n_samples = 20\n",
    "kl_weight = 1.0\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our proposed self stabilising layer for Bayesian Neural Network\n",
    "For the stabilising prior to be effective we sample from a reparametrised, $\\tilde{q}(W)$, which is the product of the current posterior, $q(W)$, and the prior, $p(W)$. This allows the the influence of the prior on the forward pass so we can propagate cleaner signals. \n",
    "\n",
    "The other main differences between this layer and a normal Bayesian layer is the update prior function, because our prior adapts based on the current settings of the weights to stabilise the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfStabilisingLayer(nn.Module):\n",
    "    '''\n",
    "    Iteratively updating self stabilising prior.\n",
    "    Fully factorised Gaussian priors and posteriors.\n",
    "    Local reparametrisation trick.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features, out_features, init_var=0.001, prior_var=0.02):\n",
    "        super(SelfStabilisingLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "\n",
    "        # initialisation values of parameters\n",
    "        self.init_var = np.log(init_var)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.log_sigma.data.fill_(self.init_var / 2)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "        # He initialisation\n",
    "        init = np.sqrt(2 / self.in_features)\n",
    "        self.W.data.normal_(0, init)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # local reparametrisation trick with new parameters from q_tilde(W)\n",
    "        # i.e. use self.new_mu and self.new_sigma_sq\n",
    "        lrt_mean = F.linear(x, self.new_mu) + self.bias\n",
    "        lrt_std = torch.sqrt(F.linear(x * x, self.new_sigma_sq) + 1e-8)\n",
    "        eps = lrt_std.data.new(lrt_std.size()).normal_()\n",
    "        pre_activation = lrt_mean + lrt_std * eps\n",
    "\n",
    "        return pre_activation\n",
    "    \n",
    "\n",
    "    def update_prior(self):\n",
    "        \n",
    "        #####################################################################\n",
    "        # Main difference between normal BNN and Stabilising prior\n",
    "        #####################################################################\n",
    "\n",
    "        # Sum of all incoming nodes to specific hidden units\n",
    "        mu_L = torch.sum(self.W, dim=1)\n",
    "        sig_sq_L = torch.sum(torch.exp(self.log_sigma * 2.0), dim=1)\n",
    "\n",
    "        # PRIOR VARIANCE        \n",
    "        gamma = 2- (1-1/math.pi) *mu_L * mu_L\n",
    "        self.prior_var = (gamma * sig_sq_L)/(sig_sq_L - gamma)\n",
    "        self.prior_var = self.prior_var / self.in_features\n",
    "\n",
    "        # shared prior across all weights feeding into the same hidden unit\n",
    "        self.prior_var = self.prior_var.expand(self.in_features, self.prior_var.shape[0]).t()\n",
    "        self.prior_var = torch.abs(self.prior_var)\n",
    "        \n",
    "        \n",
    "        # PRIOR MEAN (mean preserving)\n",
    "        self.prior_mean = self.W\n",
    "\n",
    "        \n",
    "        # PRODUCT, set the parameters from which we will be sampling q_tilde(W)\n",
    "        self.new_mu, self.new_sigma_sq = multipy_gaussian(self.W, self.prior_mean, \n",
    "                                                            torch.exp(self.log_sigma * 2.0), self.prior_var)\n",
    "\n",
    "        \n",
    "    def kl_reg(self):\n",
    "\n",
    "        # cross entropy term\n",
    "        sigma_sq = torch.exp(self.log_sigma.view(-1) * 2)\n",
    "        new_sigma_sq = torch.exp(self.new_sigma_sq.view(-1))\n",
    "        pi = math.pi\n",
    "\n",
    "        H = 0.5 * torch.log(2 * pi * sigma_sq) + (new_sigma_sq / sigma_sq)\n",
    "        H = torch.sum(H)\n",
    "\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Bayesian Neural Network layer\n",
    "Non-conjugate Gaussian prior and Gaussian posterior. We also make use of the Local Reparametrisation trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalReparametrisationLayer(nn.Module):\n",
    "    '''\n",
    "    Doubly stochastic Variational Bayes for non-conjugate inference.\n",
    "    Fully factorised Gaussian priors and posteriors.\n",
    "    Local reparametrisation trick.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, init_var=0.001, prior_var=0.0001):\n",
    "        super(LocalReparametrisationLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "\n",
    "        # add priors \n",
    "        self.prior_mean = torch.Tensor([0]).to(device)\n",
    "        self.prior_var = torch.Tensor([prior_var]).to(device)\n",
    "\n",
    "        # initialisation values of parameters\n",
    "        self.init_var = np.log(init_var)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.log_sigma.data.fill_(self.init_var / 2)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "        # critical initialisation for normal Bayesian Neural networks\n",
    "        init = np.sqrt(np.abs((2 - self.in_features * np.exp(self.init_var)) / self.in_features))\n",
    "        self.W.data.normal_(0, init)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # local reparametrisation trick \n",
    "        lrt_mean = F.linear(x, self.W) #+ self.bias\n",
    "        lrt_std = torch.sqrt(F.linear(x * x, torch.exp(self.log_sigma * 2.0)) + 1e-8)\n",
    "        eps = lrt_std.data.new(lrt_std.size()).normal_()\n",
    "        pre_activation = lrt_mean + lrt_std * eps\n",
    "\n",
    "        if self.training:\n",
    "            self.signal_variance = pre_activation.var(dim=1)[0].data.cpu().numpy()          \n",
    "\n",
    "        return pre_activation\n",
    "    \n",
    "    def kl_reg(self):\n",
    "\n",
    "        # KL divergence \n",
    "        mean = self.W.view(-1)\n",
    "        sigma = torch.exp(self.log_sigma).view(-1)\n",
    "\n",
    "        prior_sigma = torch.sqrt(self.prior_var).view(-1)\n",
    "        prior_mean = self.prior_mean.view(-1)\n",
    "\n",
    "        p = torch.distributions.normal.Normal(prior_mean, prior_sigma)\n",
    "        q = torch.distributions.normal.Normal(mean, sigma)\n",
    "\n",
    "        kl = torch.distributions.kl.kl_divergence(q, p)\n",
    "\n",
    "        kl = torch.sum(kl)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic ReLU network architecture where we specify the type of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mGfHQ4Nv8Ha"
   },
   "outputs": [],
   "source": [
    "# Define a simple fully connected ReLU Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_type, input_size, width=256, init_var=0.001):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc_in = layer_type(input_size, width, init_var=init_var)\n",
    "        self.fc_h1 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h2 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h3 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_out = layer_type(width,  output_size, init_var=init_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_in(x))\n",
    "        x = F.relu(self.fc_h1(x))\n",
    "        x = F.relu(self.fc_h2(x))\n",
    "        x = F.relu(self.fc_h3(x))\n",
    "        x = F.log_softmax(self.fc_out(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def update_priors(self):\n",
    "        if hasattr(self.fc_h1, 'update_prior'):\n",
    "            self.fc_in.update_prior()\n",
    "            self.fc_out.update_prior()\n",
    "            self.fc_h1.update_prior()\n",
    "            self.fc_h2.update_prior()\n",
    "            self.fc_h3.update_prior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define New Loss Function -- SGVLB \n",
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size, batch_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = batch_size / train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, output, target, kl_weight=1.0):\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "        kl = kl * self.num_batches\n",
    "        kl = kl / (self.batch_size*self.train_size)\n",
    "        #kl = kl_weight * kl\n",
    "        return F.nll_loss(output, target, reduction='mean') + kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fi-O-SFv8Hc"
   },
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipy_gaussian(mean1, mean2, var1, var2):\n",
    "\n",
    "    # calculate variance\n",
    "    new_var = 1 / ((1 / var1) + (1 / var2))\n",
    "\n",
    "    # calculate mean\n",
    "    new_mu = new_var * (mean2 / var2 + mean1 / var1)\n",
    "\n",
    "    return new_mu, new_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with logging\n",
    "def train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.train() \n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            #####################################################################\n",
    "            # Training\n",
    "            #####################################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, input_shape)\n",
    "              \n",
    "            # update priors\n",
    "            model.update_priors()\n",
    "\n",
    "            # forward prop\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "\n",
    "            # backprop\n",
    "            loss = loss_fn(output, target, kl_weight)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # training loss and accuracy (training accuracy does not reflect ensemble)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += torch.sum(pred.eq(target))\n",
    "\n",
    "        #####################################################################\n",
    "        # Evaluate on test set\n",
    "        #####################################################################\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, avg_test_acc, total_brier = 0, 0, 0\n",
    "        total_90, total_70, total_50 = 0, 0, 0\n",
    "        for tbatch_idx, (data, target) in enumerate(test_loader):\n",
    "            # prep\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, input_shape)\n",
    "\n",
    "            # average over weights with samples of different parameters\n",
    "            probs, test_loss = 0, 0\n",
    "            for i in range(n_samples):\n",
    "                output = model(data)\n",
    "                probs += output.data\n",
    "                test_loss += float(loss_fn(output, target).item())\n",
    "            mean_probs = probs / n_samples\n",
    "            avg_test_loss = test_loss / n_samples\n",
    "            pred = torch.argmax(mean_probs, dim=1)\n",
    "            avg_test_acc += torch.sum(pred == target)\n",
    "\n",
    "        \n",
    "\n",
    "        #####################################################################\n",
    "        # Logging\n",
    "        #####################################################################\n",
    "        # log training and test loss and accuracy\n",
    "        logger.add_scalar(epoch, 'trlos', train_loss)\n",
    "        logger.add_scalar(epoch, 'telos', avg_test_loss)\n",
    "\n",
    "        logger.add_scalar(epoch, 'tracc', (float(train_acc) / (batch_size * (batch_idx + 1)) * 100))\n",
    "        logger.add_scalar(epoch, 'teacc', float(avg_test_acc) / len(test_loader.dataset) * 100)\n",
    "\n",
    "        logger.iter_info()\n",
    "    logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "A very simple example on MNIST to demonstrate accelerated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 512\n",
    "init_var = 0.01\n",
    "epochs = 10\n",
    "input_shape = 28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normal BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7HkpvRVv8Hh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step    trlos    telos    tracc    teacc\n",
      "------  -------  -------  -------  -------\n",
      "     1  2010.57     1.87    15.36    23.09\n",
      "     2   950.00     1.28    41.93    54.99\n",
      "     3   728.50     1.22    55.36    62.84\n",
      "     4   458.09     0.50    74.33    91.99\n",
      "     5   233.44     0.24    90.56    93.72\n",
      "     6   189.43     0.30    92.45    94.55\n",
      "     7   164.02     0.32    93.33    95.32\n",
      "     8   140.17     0.17    94.41    95.30\n",
      "     9   124.87     0.28    95.07    95.85\n",
      "    10   113.41     0.28    95.53    96.27\n",
      "The log/output have been saved to: ./logs//LRTNet-otb-05-22-12:52 + .csv/.out/\n"
     ]
    }
   ],
   "source": [
    "model = Net(LocalReparametrisationLayer, input_shape, width=width, init_var=init_var)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "logger = Logger(name='LRTNet')\n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size=batch_size)\n",
    "loss_fn = SGVLB(model, len(train_loader.dataset), batch_size)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model.cuda()\n",
    "    \n",
    "train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Self stabilising prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1730
    },
    "colab_type": "code",
    "id": "v6mf7WjhJIqA",
    "outputId": "20da9957-22f0-45d9-faa8-705ca4b58a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step    trlos    telos    tracc    teacc\n",
      "------  -------  -------  -------  -------\n",
      "     1   346.23     0.24    84.03    94.91\n",
      "     2   147.74     0.14    93.84    96.65\n",
      "     3   116.52     0.15    95.38    97.07\n",
      "     4    95.47     0.24    96.46    97.29\n",
      "     5    84.51     0.14    96.97    97.61\n",
      "     6    73.30     0.15    97.53    97.45\n",
      "     7    66.33     0.07    97.76    97.83\n",
      "     8    61.38     0.14    98.07    97.83\n",
      "     9    56.38     0.09    98.21    98.08\n",
      "    10    53.54     0.12    98.28    97.82\n",
      "The log/output have been saved to: ./logs//StabilisedNet-xne-05-22-12:55 + .csv/.out/\n"
     ]
    }
   ],
   "source": [
    "kl_weight = 1.0\n",
    "\n",
    "model = Net(SelfStabilisingLayer, input_shape, width=width, init_var=init_var)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "logger = Logger(name='StabilisedNet')\n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size=batch_size)\n",
    "loss_fn = SGVLB(model, len(train_loader.dataset), batch_size)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model.cuda()\n",
    "    \n",
    "train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3. Sparse Variational Dropout.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
